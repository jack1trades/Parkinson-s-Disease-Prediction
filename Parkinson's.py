# -*- coding: utf-8 -*-
"""Parkinson's.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12wUdPXEg59YNBgoOWY8KF4dH705i5OWn

**PARKINSON'S DISEASE**

**The first Neurological syndrome to be ever discovered**

**With its first medical description written in 1817, finding the right medicine is still a challenge in medical world**

**However, we can garner much comfort in the following words:**

*   **"Prevention is better than cure"**

**Data Science aids in a number of ways to predict this syndrome based on patient data, beforehand.**

##### **Dataset**
"""

import pandas as pd

ps = pd.read_csv("parkinsons_data.csv")

ps.head()

# Renaming column variable names

ps = ps.rename(columns = {       "MDVP:Flo(Hz)"      :   "MDVP_Flo(Hz)",
                                 "MDVP:Fhi(Hz)"      :   "MDVP_Fhi(Hz)",
                                 "MDVP:Fo(Hz)"      :   "MDVP_Fo(Hz)",
                                 "MDVP:Jitter(%)"    :   "MDVP_Jitter(percentage)",
                                 "MDVP:Jitter(Abs)"  :   "MDVP_Jitter(Abs)",
                                 "MDVP:RAP"          :   "MDVP_RAP",
                                 "MDVP:PPQ"          :   "MDVP_PPQ",
                                 "Jitter:DDP"        :   "Jitter_DDP",
                                 "MDVP:Shimmer"      :   "MDVP_Shimmer",
                                 "MDVP:Shimmer(dB)"  :   "MDVP_Shimmer(dB)",
                                 "Shimmer:APQ3"      :   "Shimmer_APQ3",
                                 "Shimmer:APQ5"      :   "Shimmer_APQ5",
                                 "MDVP:APQ"          :   "MDVP_APQ",
                                 "Shimmer:DDA"       :   "Shimmer_DDA"      })

ps.head()

ps.shape

ps.isnull().sum().sum()

ps.dtypes

ps.info()

from collections import Counter
Counter(ps["status"])

ps = ps.drop(columns=["name"], axis=1)

"""**Column Attributes**

*   name - ASCII subject name and recording number item
*   MDVP:Fo(Hz) - Average vocal fundamental frequency
*   MDVP:Fhi(Hz) - Maximum vocal fundamental frequency
*   MDVP:Flo(Hz) - Minimum vocal fundamental frequency
*   MDVP:Jitter(%),MDVP:Jitter(Abs),MDVP:RAP,MDVP:PPQ,Jitter:DDP - Several measures of variation in fundamental frequency
*   MDVP:Shimmer,MDVP:Shimmer(dB),Shimmer:APQ3,Shimmer:APQ5,MDVP:APQ,Shimmer:DDA - Several measures of variation in amplitude
*   NHR,HNR - Two measures of ratio of noise to tonal components in the voice
*   status - Health status of the subject (one) - Parkinson's, (zero) - healthy
*   RPDE,D2 - Two nonlinear dynamical complexity measures
*   DFA - Signal fractal scaling exponent
*   spread1,spread2,PPE - Three nonlinear measures of fundamental frequency variation

##### **Dataset visualizations**
"""

import matplotlib.pyplot as plt
import seaborn as sns

plt.hist(ps["MDVP_Fo(Hz)"])

plt.hist(ps["MDVP_Fhi(Hz)"])



"""##### **Scaling**"""

from sklearn.preprocessing import StandardScaler

X = ps.drop(columns=["status"], axis=1)
y = ps.status

stdscaler = StandardScaler()
scaled_features = stdscaler.fit_transform(X)
scaled_features_df = pd.DataFrame(scaled_features, columns = X.columns)

ps = scaled_features_df.join(y)

ps.head()

ps.isna().sum().sum()

"""##### **Train-Test-Split**"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)

X_train.shape, X_test.shape, y_train.shape, y_test.shape

"""##### **Linear Regression**"""

from sklearn.linear_model import LinearRegression

linear = LinearRegression()

linear.fit(X_train, y_train)

# prediction

y_pred = linear.predict(X_test)

y_pred

# classification metrics

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

print("1. Accuracy Score of  - ", accuracy_score(y_pred.round(), y_test))
print("\n")
print("2. Confusion Matrix - \n", confusion_matrix(y_pred.round(), y_test))
print("\n")
print("3. Classification_report - \n\n", classification_report(y_pred.round(), y_test))

"""Confusion Matrix:

*   TP FP
*   FN TN


"""

linear.score(X_test, y_pred.round())

linear.score(X_test, y_pred.round())

y_test.shape, y_pred.round().shape

# for i,j in enumerate(y_pred):
#   if

import numpy as np
o = [0.77, 4.65, 0.56, 0.51]
k = np.array(o)
k.round()

Counter(y_test)

Counter(y_pred.round())

"""*   **43 cases of Parkinson's in test data**
*   **48 cases of Parkinson's in pred data**



*   **16 cases of non-Parkinson's in test data**
*   **11 cases of non-Parkinson's in pred data**


"""

Counter(np.abs(y_test - y_pred.round()))

"""**As said theoretically, linear regression don't do well on non-linear variables**

**Check these** 


*   https://stats.stackexchange.com/questions/405961/is-there-formal-test-of-non-linearity-in-linear-regression
*   https://www.pluralsight.com/guides/non-linear-regression-trees-scikit-learn

##### **Logistic Regression**
"""

from sklearn.linear_model import LogisticRegression

logistic = LogisticRegression()

logistic.fit(X_train, y_train)

# Prediction

y_pred = logistic.predict(X_test)

y_pred

# Classification Report

# classification metrics

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

print("1. Accuracy Score of  - ", accuracy_score(y_pred, y_test))
print("\n")
print("2. Confusion Matrix - \n", confusion_matrix(y_pred, y_test))
print("\n")
print("3. Classification_report - \n\n", classification_report(y_pred, y_test))

logistic.score(X_test, y_test)



"""**Non-Linear Regression**



*   DT
*   RF
*   SVM
*   kNN

##### **Support Vector Machines**
"""

from sklearn.svm import SVC

svm = SVC()
svm.fit(X_train, y_train)

# Prediction

y_pred = svm.predict(X_test)



y_pred

# classification metrics

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

print("1. Accuracy Score of  - ", accuracy_score(y_pred, y_test))
print("\n")
print("2. Confusion Matrix - \n", confusion_matrix(y_pred, y_test))
print("\n")
print("3. Classification_report - \n\n", classification_report(y_pred, y_test))

svm.score(X_test, y_test)

"""**Hyperparameter Tuning with the aid of GridSearchCV**"""

# Hyperparameter tuning - GridSearchCV

from sklearn.model_selection import GridSearchCV

param_grid = {
              'C' : [0.1, 1, 10, 100, 1000],
              'gamma' : [1, 0.1, 0.01, 0.001, 0.0001],
              'gamma' : ['scale', 'auto'],
              'kernel' : ['linear', 'rbf', 'poly', 'sigmoid', 'nonlinear']
              }

grid = GridSearchCV(svm, param_grid, refit=True, cv=3, verbose=0)   # n_jobs=

grid.fit(X_train, y_train)

grid.best_params_

grid.best_estimator_

# Predictions

y_pred_grid = grid.predict(X_test)
y_pred_grid

# classification metrics

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

print("1. Accuracy Score of  - ", accuracy_score(y_pred_grid, y_test))
print("\n")
print("2. Confusion Matrix - \n", confusion_matrix(y_pred_grid, y_test))
print("\n")
print("3. Classification_report - \n\n", classification_report(y_pred_grid, y_test))

"""**As seen above, despite GridSearhCV, SVM makes us say, "Nay! We can do better!"**

##### **Ad : Irrelevant yet relevant**
"""

cman = ['status']
xpxp = pd.read_csv("parkinsons_data.csv")[cman]

xpxp.head()

## Whilst working on this -- got bored and --
## did some googling -- which you are about to find out --
## !!

# +--------------------------------+
# | *** Ways to handle big data ** |
# +--------------------------------+
# 
# 1. Select a subset of your data
# 1.1 Study, Clean, and make a baseline model
# 
# 2. Load only the columns you need
# 2.1 Domain knowledge
# 
# 3. Use dtypes efficiently. i.e., df.memory_usage
# 3.1 pd.to_numeric(df["id"], downcast="unsigned")
# 3.2 Nominals --> .astype("category")
# 3.3   Floats --> .apply(pd.to_numeric, downcast="float")
# 3.0 Compare with df.memory_usage
# 
# 4. Parallelize model-training using more processing cores
# 4.1 By def, sklearn uses 1 core, despite a computer has >=4 cores
# 4.2 Use "n_jobs=-1" for that
# 
# 5. Pickle or feathers format
# 5.1. Faster reading and writing
# 
# 6. Speed up pandas operations using - 
# 6.1 - "pd.eval"
#
# ARTICULO FINITOO!!



"""##### **Decision Trees**"""

from sklearn.tree import DecisionTreeClassifier

tree = DecisionTreeClassifier(criterion="entropy",  
                              random_state=0)

tree.fit(X_train, y_train)

# Prediction

y_pred = tree.predict(X_test)

# classification metrics

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

print("1. Accuracy Score of  - ", accuracy_score(y_pred, y_test))
print("\n")
print("2. Confusion Matrix - \n", confusion_matrix(y_pred, y_test))
print("\n")
print("3. Classification_report - \n\n", classification_report(y_pred, y_test))

# Attributes

print("\t---------------------------------------")
print("\t**Attributes of DeisionTreeClassifier**")
print("\t---------------------------------------")
print('\n')
print("1. Class labels of target -", tree.classes_)
print("\n")

d = dict();
column_list = X.columns

for i in range(len(tree.feature_importances_)):
  d[column_list[i]]= tree.feature_importances_[i]

print("2. Number of features involved -", tree.n_features_in_)
print("\n")

print("3. Feature Importances -\n", d)
print("\n")

print("Number of outputs when fit is performed -", tree.n_outputs_)



"""**And, Remember...**



*   **model.score(X_test, y_test) gives test score accuracy**
*   **model.sklearn.metrics.accuracy_score(X_test, y_test) gives overall model accuracy**


"""



"""##### **k- Nearest Neighbors**

**(Hush!! Don't tell anybody: kNN is a non-parametric method)**
"""

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier()

knn.fit(X_train, y_train)

# Prediction

y_pred = knn.predict(X_test)

# classification metrics

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

print("1. Accuracy Score of  - ", accuracy_score(y_pred, y_test))
print("\n")
print("2. Confusion Matrix - \n", confusion_matrix(y_pred, y_test))
print("\n")
print("3. Classification_report - \n\n", classification_report(y_pred, y_test))

"""**You guessed it!!**

**Hyperparameter Tuning, it is**
"""

# Hyperparamter tuning to fit the model with best parameters

from sklearn.model_selection import GridSearchCV

leaf_size   = list(range(1,50))
n_neighbors = list(range(1,20))
p = [1,2]

param_grid = dict(leaf_size = leaf_size,
                  n_neighbors = n_neighbors,
                  p = p)

grid = GridSearchCV(knn, param_grid, cv=5 )  # verbose=0, n_job=

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# grid.fit(X_train, y_train)

grid.best_params_
grid.best_estimator_

# Prediction

y_pred_grid = grid.predict(X_test)

# classification metrics

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

print("1. Accuracy Score of  - ", accuracy_score(y_pred_grid, y_test))
print("\n")
print("2. Confusion Matrix - \n", confusion_matrix(y_pred_grid, y_test))
print("\n")
print("3. Classification_report - \n\n", classification_report(y_pred_grid, y_test))

"""**Despite Hyperparameter tuning, kNN's best isn't our best**"""



"""##### **Random Forest Classifier**"""

from sklearn.ensemble import RandomForestClassifier

rfc = RandomForestClassifier()

rfc.fit(X_train, y_train)

# Prediction

y_pred = rfc.predict(X_test)

# classification metrics

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

print("1. Accuracy Score of  - ", accuracy_score(y_pred, y_test))
print("\n")
print("2. Confusion Matrix - \n", confusion_matrix(y_pred, y_test))
print("\n")
print("3. Classification_report - \n\n", classification_report(y_pred, y_test))

"""

*   **"Be content with what you have" can be translated as "Be content with --Whatever"... Yeah! "Whatever", when you don't strive for better from good or from better to the best**
*   ***Not the destiny; The journey do -- yeah -- matters***"""

# Hyperparameter Tuning of RFC

# RandomizedSearchCV

from sklearn.model_selection import RandomizedSearchCV

param_distributions = {
                      'bootstrap':[True, False],
                      'max_depth':[80, 90, 100, 110, 120],
                      'max_features': ['auto', 'sqrt'],
                      'min_samples_leaf':[1,2,3,4,5,6,7,8,9,10],
                      'min_samples_split':[2, 3, 5, 8, 10, 12, 14, 15],
                      'n_estimators':[200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]
                      }

randCV = RandomizedSearchCV( 
                     estimator = rfc, 
                     param_distributions = param_distributions,
                     cv = 3,
                     n_iter = 100
                    )                                                             # verbose = , n_jobs =

import os
  
n_cpu = os.cpu_count()
print("Number of CPUs in the system:", n_cpu)

# rfc.get_params()  .keys()

rfc.get_params().keys()

randCV.fit(X_train, y_train)

randCV.best_params_

# Prediction

y_pred_rand = randCV.predict(X_test)

# classification metrics

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

print("1. Accuracy Score of  - ", accuracy_score(y_pred_rand, y_test))
print("\n")
print("2. Confusion Matrix - \n", confusion_matrix(y_pred_rand, y_test))
print("\n")
print("3. Classification_report - \n\n", classification_report(y_pred_rand, y_test))

"""**THE SAME**"""



"""##### **AdaBoostClassifier**"""

from sklearn.ensemble import AdaBoostClassifier

ada = AdaBoostClassifier()

ada.fit(X_train, y_train)

# Prediction

y_pred = ada.predict(X_test)

# classification metrics

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

print("1. Accuracy Score of  - ", accuracy_score(y_pred, y_test))
print("\n")
print("2. Confusion Matrix - \n", confusion_matrix(y_pred, y_test))
print("\n")
print("3. Classification_report - \n\n", classification_report(y_pred, y_test))



"""##### **GradientBoostingClassifier**"""

from sklearn.ensemble import GradientBoostingClassifier

gradyb = GradientBoostingClassifier()

gradyb.fit(X_train, y_train)

# Prediction

y_pred = gradyb.predict(X_test)

# classification metrics

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

print("1. Accuracy Score of  - ", accuracy_score(y_pred, y_test))
print("\n")
print("2. Confusion Matrix - \n", confusion_matrix(y_pred, y_test))
print("\n")
print("3. Classification_report - \n\n", classification_report(y_pred, y_test))



"""##### **XGBClassifier**"""

import xgboost
from xgboost import XGBClassifier

xgb = XGBClassifier()

xgb.fit(X_train, y_train)

# Prediction

y_pred = xgb.predict(X_test)

# classification metrics

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

print("1. Accuracy Score of  - ", accuracy_score(y_pred, y_test))
print("\n")
print("2. Confusion Matrix - \n", confusion_matrix(y_pred, y_test))
print("\n")
print("3. Classification_report - \n\n", classification_report(y_pred, y_test))

"""**94.9% seeems a good percentage.**

**But, it's always a good time to say "Why not?"**
"""



"""##### **Inference**

**Based on the accuracy scores, we can infer that following classifiers has high accuracies -**

1.   **XGBoostingClassifier, GradientBoostingClassifier  :  94.9%**
2.   **DecisionTreeClassifier, RandomForestClassifier    : 93.2%**

##### **Deployment**
"""

# Pckle dump

import pickle

pickle.dump(xgb, open("xgb.pkl", "wb"))

# Pydantic

!pip install pydantic
from pydantic import BaseModel

class patient_details(BaseModel):
  MDVP_Fo(Hz)      :   float
  MDVP_Fhi(Hz)     :   float
  MDVP_Flo(Hz)     :   float
  MDVP_Jitter(percentage)   :   float
  MDVP_itter(Abs)  :   float
  MDVP_RAP         :   float
  MDVP_PPQ         :   float
  Jitter_DDP       :   float
  MDVP_Shimmer     :   float
  MDVP_Shimmer(dB) :   float
  Shimmer_APQ3     :   float
  Shimmer_APQ5     :   float
  MDVP_APQ         :   float
  Shimmer_DDA      :   float
  NHR              :   float
  HNR              :   float
  RPDE             :   float
  DFA              :   float
  spread1          :   float
  spread2          :   float
  D2               :   float
  PPE              :   float

  class Config:
    schema_extra = {
        "example" : {
            "MDVP_Fo(Hz)"      :   789.123,
            "MDVP_Fhi(Hz)"     :   564.987,
            "MDVP_Flo(Hz)"     :   123.456,
            "MDVP_Jitter(%)"   :   0.01234,
            "MDVP_Jitter(Abs)" :   0.45678,
            "MDVP_RAP"         :   0.00589,
            "MDVP_PPQ"         :   0.00548,
            "Jitter_DDP"       :   0.02145,
            "MDVP_Shimmer"     :   0.05896,
            "MDVP_Shimmer(dB)" :   0.426,
            "Shimmer_APQ3"     :   0.02588,
            "Shimmer_APQ5"     :   0.03258,
            "MDVP_APQ"         :   0.02895,
            "Shimmer_DDA"      :   0.06895,
            "NHR"              :   0.02866,
            "HNR"              :   25.895,
            "RPDE"             :   0.414986,
            "DFA"              :   0.123456,
            "spread1"          :   0.123456,
            "spread2"          :   0.123456,
            "D2"               :   0.123456,
            "PPE"              :   0.123456
        }
    }

# FastAPI

!pip install fastapi
from fastapi import FastAPI

app = FastAPI()

@app.on_event("startup")
def load_model():                                   # path.operation.functiuno
  global model
  model = pickle.load(open("xgb.pkl", "rb"))

@app.get("/")
def index():
  return {"message" : "Homepage of API"}

@app.post("predict")
def get_patient_details(data: patient_details):
  received = data.dict()
  MDVP_Fo(Hz)  =  received["MDVP_Fo(Hz)"]
  MDVP_Fhi(Hz)  =  received["MDVP_Fhi(Hz)"]
  MDVP_Flo(Hz)  =  received["MDVP_Flo(Hz)"]
  MDVP_Jitter(percentage)  =  received["MDVP_Jitter(percentage)"]
  MDVP_Jitter(Abs)  =  received["MDVP_Jitter(Abs)"]
  MDVP_RAP  =  received["MDVP_RAP"]
  MDVP_PPQ  =  received["MDVP_PPQ"]
  Jitter_DDP  =  received["Jitter_DDP"]
  MDVP_Shimmer  =  received["MDVP_Shimmer"]
  MDVP_Shimmer(dB)  =  received["MDVP_Shimmer(dB)"]
  Shimmer_APQ3  =  received["Shimmer_APQ3"]
  Shimmer_APQ5  =  received["Shimmer_APQ5"]
  MDVP_APQ  =  received["MDVP_APQ"]
  Shimmer_DDA  =  received["Shimmer_DDA"]
  RPDE  =  received["RPDE"]
  DFA  =  received["DFA"]
  spread1  =  received["spread1"]
  spread2  =  received["spread2"]
  D2  =  received["D2"]
  PPE  =  received["PPE"]
  
  pred_name = model.predict([[Gender, Married, Dependents, Education, Self_Employed, ApplicantIncome, CoapplicantIncome, LoanAmount, Loan_Amount_Term, Credit_History, Property_Area]]).tolist()[0]
  return {"prediction" : pred_name}

# ColabCode

!pip install colabcode

from colabcode import ColabCode

server = ColabCode(port = 15000, code=False)

server.run_app(app = app)

"""##### **END OF THE LINE**"""